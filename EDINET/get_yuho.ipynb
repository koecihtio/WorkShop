{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 有報キャッチャーから有価証券報告書を取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.select import Select\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 決算短信のURL一覧を取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_day(driver, day):\n",
    "    ID_START = 'ctl00_contentsCopy_IrList1_TextBoxFrom'\n",
    "    ID_END = 'ctl00_contentsCopy_IrList1_TextBoxTo'\n",
    "    \n",
    "    for id in [ID_START, ID_END]:\n",
    "        element = driver.find_element_by_id(id)\n",
    "        element.clear()\n",
    "        element.send_keys(day)\n",
    "    \n",
    "    search_btn = driver.find_element_by_id('ctl00_contentsCopy_IrList1_ButtonSearch2')\n",
    "    driver.find_element_by_id(ID_START).click()\n",
    "    search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doctitle_url(element, doc_num):\n",
    "    code_id = 'ctl00_contentsCopy_IrList1_ListViewIR_ctrl' + str(doc_num)+ '_HyperLinkCompany'\n",
    "    code_element = element.find_element_by_id(code_id)\n",
    "    code = re.search(r'\\d+', code_element.text).group(0)\n",
    "\n",
    "    ym_id = 'ctl00_contentsCopy_IrList1_ListViewIR_ctrl' + str(doc_num) + '_lblDisclosureDate'\n",
    "    ym_element = element.find_element_by_id(ym_id)\n",
    "    ym = ym_element.text.replace('/', '')\n",
    "\n",
    "    doc_element_id = 'ctl00_contentsCopy_IrList1_ListViewIR_ctrl' + str(doc_num) + '_lblDocName'\n",
    "    doc_element = element.find_element_by_id(doc_element_id)\n",
    "    doc_title = doc_element.text\n",
    "\n",
    "    title = '_'.join([code, ym, doc_title])\n",
    "    url = doc_element.get_attribute('href')\n",
    "    result = [title, url]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_day_list(start_date, end_date):\n",
    "    \"\"\"開始日時と終了日時を設定すると、期間内の日付がすべて入ったリストが返される\n",
    "\n",
    "    Args:\n",
    "        start_date (datetime.date): 開始日時\n",
    "        end_date (datetime.date): 終了日時\n",
    "\n",
    "    Returns:\n",
    "        list: 開始日時から終了日時までのすべての日付が格納されたリスト\n",
    "    \"\"\"\n",
    "    period = int((end_date - start_date).days)\n",
    "    day_list = []\n",
    "    for d in range(period):\n",
    "        day = start_date + datetime.timedelta(days=d)\n",
    "        day = str(day).replace('-', '/')\n",
    "        day_list.append(day)\n",
    "    \n",
    "    day_list.append(str(end_date).replace('-', '/'))\n",
    "    print(f'start_date is {day_list[0]}')\n",
    "    print(f'end_date is {day_list[-1]}')\n",
    "    print(f'period is {period} days')\n",
    "\n",
    "    return day_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pageinfo(driver, day):\n",
    "    try:\n",
    "        # 検索対象が見つかった場合\n",
    "        table = driver.find_element_by_id('ctl00_contentsCopy_IrList1_ListViewIR_tblProducts')\n",
    "        trs = table.find_elements_by_tag_name('tr')\n",
    "\n",
    "        titles = []\n",
    "        urls = []\n",
    "        for i, e in enumerate(trs):\n",
    "            doc_info = get_doctitle_url(e, i)\n",
    "            titles.append(doc_info[0])\n",
    "            urls.append(doc_info[1])\n",
    "        \n",
    "        df = pd.DataFrame(columns=['title', 'url'])\n",
    "        df['title'] = titles\n",
    "        df['url'] = urls\n",
    "        print(f'{day}に提出された書類のURL取得完了')\n",
    "        return df\n",
    "\n",
    "    except:\n",
    "        # 検索対象が見つからなかった場合\n",
    "        print(f\"{day}に提出された有価証券報告書は存在しません。\")\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dayinfo(driver, day):\n",
    "    display_day(driver, day)\n",
    "\n",
    "    try:\n",
    "        page_change_id = 'ctl00_contentsCopy_IrList1_DropDownListPage'\n",
    "        page_change_element = driver.find_element_by_id(page_change_id)\n",
    "        select = Select(page_change_element)\n",
    "        selectOps = select.options\n",
    "\n",
    "        df_list = []\n",
    "        for o in selectOps:\n",
    "            if o.text == '1ページ':\n",
    "                df = get_pageinfo(driver, day)\n",
    "            else:\n",
    "                select.select_by_visible_text(o.text)\n",
    "                df = get_pageinfo(driver, day)\n",
    "            \n",
    "            df_list.append(df)\n",
    "    except:\n",
    "        df_list = [get_pageinfo(driver, day)]\n",
    "    \n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yuho_url(start, end):\n",
    "    url = 'https://ufocatch.com/Xir.aspx?m=y'\n",
    "    driver = webdriver.Chrome('../driver/chromedriver.exe')\n",
    "    driver.get(url)\n",
    "    search_box = driver.find_element_by_id('ctl00_contentsCopy_IrList1_TextBoxSearch')\n",
    "    search_box.send_keys('有価証券報告書')\n",
    "\n",
    "    for year in range(start, end):\n",
    "        start = datetime.date(year, 1, 1)\n",
    "        end = datetime.date(year, 12, 31)\n",
    "        period = int((end - start).days) + 1\n",
    "        day_list = make_day_list(start, end)\n",
    "\n",
    "        df_list = []\n",
    "        count = 1\n",
    "        for day in day_list:\n",
    "            print(f'now loading: {day}, 進捗率: {count}/{period}')\n",
    "            df_list_1day = get_dayinfo(driver, day)\n",
    "            count += 1\n",
    "            df_list.append(df_list_1day)\n",
    "\n",
    "        df_list_year = [d2 for d in df_list for d2 in d if not d2 is None]\n",
    "        df_all = pd.concat(df_list_year)\n",
    "\n",
    "        df_all.reset_index(inplace=True)\n",
    "        df_all.drop(columns='index', inplace=True)\n",
    "\n",
    "        file_path = '../../data/EDINET/download_list/' + str(year) +\\\n",
    "             '_yuhochatcher_dl_list.pickle'\n",
    "        with open(file_path, mode='wb') as file:\n",
    "            pickle.dump(df_all, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 2014\n",
    "end = 2017\n",
    "get_yuho_url(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_dl():\n",
    "    os.chdir('C:/Users/koeci/Google ドライブ/MBA/ワークショップ/data/EDINET/download_list')\n",
    "    dl_list = []\n",
    "    for year in range(2008, 2017):\n",
    "        path = str(year) + '_yuhochatcher_dl_list.pickle'\n",
    "        with open(path, mode='rb') as file:\n",
    "            dl = pickle.load(file)\n",
    "            dl_list.append(dl)\n",
    "\n",
    "    dl_all = pd.concat(dl_list, axis=0)\n",
    "    dl_all.reset_index(inplace=True)\n",
    "    dl_all.to_csv('yuhochatcher_all_dl_list.csv', encoding='CP932')\n",
    "    with open('yuhochatcher_all_dl_list.pickle', mode='wb') as file:\n",
    "        pickle.dump(dl_all, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_all_dl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ダウンロードする際の正しい書類名、URLに修正する\n",
    "def fix_url(start, end):\n",
    "    os.chdir('C:/Users/koeci/Google ドライブ/MBA/ワークショップ/data/EDINET/download_list')\n",
    "    df_list = []\n",
    "    for year in range(start, end):\n",
    "        path = str(year) + '_yuhochatcher_dl_list.pickle'\n",
    "        with open(path, mode='rb') as file:\n",
    "            df = pickle.load(file)\n",
    "        \n",
    "        df = df[~df['title'].str.contains('訂正') \\\n",
    "            & ~df['title'].str.contains('修正') \\\n",
    "            & ~df['title'].str.contains('数値') \\\n",
    "            & ~df['title'].str.contains('データ')]\n",
    "        df['title'] = df['title'].str.replace(' ', '').str.replace('　', '')\n",
    "        \n",
    "        url_list = list(df['url'])\n",
    "        new_url_list = []\n",
    "        for url in url_list:\n",
    "            url_split = url.split('=')\n",
    "            common = 'https://resource.ufocatch.com/data/edinet/'\n",
    "            code = url_split[1].split('&')[0] # ED~~の部分だけ抜く操作\n",
    "            new_url = common + code\n",
    "            new_url_list.append(new_url)\n",
    "\n",
    "        new_df = df.drop(columns='url', inplace=False)\n",
    "        new_df['url'] = new_url_list\n",
    "\n",
    "        path = 'C:/Users/koeci/Google ドライブ/MBA/ワークショップ/data/EDINET/download_list_fixed/'\\\n",
    "             + str(year) + '_yuhochatcher_dl_list_fixed.pickle'\n",
    "        with open(path, mode='wb') as file:\n",
    "            pickle.dump(new_df, file)\n",
    "\n",
    "        df_list.append(new_df)\n",
    "\n",
    "    path = 'C:/Users/koeci/Google ドライブ/MBA/ワークショップ/data/EDINET/download_list_fixed/'\\\n",
    "             + 'yuhochatcher_all_dl_list_fixed.pickle'\n",
    "    df_all = pd.concat(df_list, axis=0)\n",
    "    df_all.reset_index(inplace=True)\n",
    "    with open(path, mode='wb') as file:\n",
    "        pickle.dump(df_all, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 2008\n",
    "end = 2017\n",
    "fix_url(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. URLの一覧から決算短信XBRLファイルをダウンロードする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.select import Select\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(year, n=0):\n",
    "    os.chdir('C:/Users/koeci/Google ドライブ/MBA/ワークショップ/data/EDINET/download_list_fixed/')\n",
    "    path = 'C:/Users/koeci/Google ドライブ/MBA/ワークショップ/data/EDINET/download_list_fixed/' + str(year) + '_yuhochatcher_dl_list_fixed.pickle'\n",
    "    with open(path, mode='rb') as file:\n",
    "        df = pickle.load(file)\n",
    "\n",
    "    all_num = df.shape[0]\n",
    "    filename_list = 'D:/Workshop_Data/securities/' + str(year) + '/' + df['title'] + '.zip'\n",
    "    url_list = df['url']\n",
    "    filename_list = filename_list[n:]\n",
    "    url_list = url_list[n:]\n",
    "    for filename, url in zip(filename_list, url_list):\n",
    "        doc = requests.get(url=url)\n",
    "        try:\n",
    "            with open(filename, mode='wb') as file:\n",
    "                file.write(doc.content)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        print(f'{year}: {n + 1} / {all_num} was saved.')\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3d1cbc4b5331>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2017\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0myear\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0msave_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'save_file' is not defined"
     ]
    }
   ],
   "source": [
    "start = 2013\n",
    "end = 2017\n",
    "for year in range(start, end):\n",
    "    save_file(year, n=0)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "530756d384d84b166bf1cc6dadec8b1c5ff774bb470199997245af3e35454b18"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
