{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XBRLファイルから株主データを取得する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. XBRLからHTML形式で株主テーブルを取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from arelle import ModelManager\n",
    "from arelle import Cntlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(zip_path):\n",
    "    zip_files = glob.glob(os.path.join(zip_path, '*.zip'))\n",
    "    for i, f in enumerate(zip_files):\n",
    "        len_files = len(zip_files)\n",
    "        print(f'file:{f}, loading {i+1} / {len_files}')\n",
    "        try:\n",
    "            with zipfile.ZipFile(f) as zip_f:\n",
    "                zip_f.extractall(zip_path)\n",
    "        except:\n",
    "            print('zipファイルではありません。')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = 'D:/Workshop_Data/new_release/'\n",
    "unzip(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xbrl(xbrl_path, intermediate_path, recursive=False):\n",
    "    edinet_code_list = []\n",
    "    security_code_list = []\n",
    "    firm_name_list = []\n",
    "    ymd_list = []\n",
    "    stk_html_list = []\n",
    "\n",
    "    xbrl_files = glob.glob(xbrl_path, recursive=recursive)\n",
    "    length = len(xbrl_files)\n",
    "\n",
    "    for i, xbrl_file in enumerate(xbrl_files):\n",
    "        print(f'{i + 1} / {length}')\n",
    "        ctrl = Cntlr.Cntlr()\n",
    "        model_manager = ModelManager.initialize(ctrl)\n",
    "        model_xbrl = model_manager.load(xbrl_file)\n",
    "\n",
    "        edinet_code = 0\n",
    "        security_code = 0\n",
    "        firm_name = 0\n",
    "        ymd = 0\n",
    "        stockholder = 0\n",
    "        for fact in model_xbrl.facts:\n",
    "            if fact.concept.qname.localName == 'EDINETCodeDEI':\n",
    "                print(f'EDINETコード：{fact.value}')\n",
    "                edinet_code = fact.value\n",
    "\n",
    "            elif fact.concept.qname.localName == 'SecurityCodeDEI':\n",
    "                print(f'証券コード：{fact.value}')\n",
    "                security_code = fact.value\n",
    "            \n",
    "            elif fact.concept.qname.localName == 'FilerNameInJapaneseDEI':\n",
    "                print(f'企業名：{fact.value}')\n",
    "                firm_name = fact.value\n",
    "\n",
    "            elif fact.concept.qname.localName == 'FilingDateCoverPage':\n",
    "                print(f'提出日：{fact.value}')\n",
    "                ymd = fact.value\n",
    "\n",
    "            elif fact.concept.qname.localName == 'SummaryOfShareholdersTextBlock':\n",
    "                # print(f'株主の状況：{fact.value}')\n",
    "                stockholder = fact.value\n",
    "\n",
    "        edinet_code_list.append(edinet_code)\n",
    "        security_code_list.append(security_code)\n",
    "        firm_name_list.append(firm_name)\n",
    "        ymd_list.append(ymd)\n",
    "        stk_html_list.append(stockholder)\n",
    "        print(edinet_code, firm_name, ymd)\n",
    "\n",
    "    df_stk = pd.DataFrame(data=[edinet_code_list, security_code_list, firm_name_list, ymd_list, stk_html_list]).T\n",
    "    df_stk.columns = ['edinet_code', 'security_code', 'firm_name', 'filling_ymd', 'stk_html']\n",
    "    with open(intermediate_path, mode='wb') as file:\n",
    "        pickle.dump(df_stk, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDINETからの取得分\n",
    "xbrl_path = 'D:/Workshop_Data/new_release/XBRL/PublicDoc/*.xbrl'\n",
    "intermediate_path = 'C:/Users/koeci/Google ドライブ/MBA/ワークショップ/intermediate/xbrl_parsed_0924.pickle'\n",
    "xbrl(xbrl_path, intermediate_path)\n",
    "\n",
    "# 有報キャッチャーからの取得分\n",
    "xbrl_path = 'D:/Workshop_Data/new_release_2/**/*.xbrl'\n",
    "intermediate_path = 'C:/Users/koeci/Google ドライブ/MBA/ワークショップ/intermediate/xbrl_parsed_1015.pickle'\n",
    "xbrl(xbrl_path, intermediate_path, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_path = 'C:/Users/koeci/Google ドライブ/MBA/ワークショップ/intermediate/xbrl_parsed_0924.pickle'\n",
    "with open(intermediate_path, mode='rb') as file:\n",
    "    df_stk_1 = pickle.load(file)\n",
    "intermediate_path = 'C:/Users/koeci/Google ドライブ/MBA/ワークショップ/intermediate/xbrl_parsed_1015.pickle'\n",
    "with open(intermediate_path, mode='rb') as file:\n",
    "    df_stk_2 = pickle.load(file)\n",
    "\n",
    "df_stk = pd.concat([df_stk_1, df_stk_2], axis=0)\n",
    "df_stk = df_stk.query('not stk_html == 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translate:\n",
    "    ZENKAKU = ''.join(chr(0xff01 + i) for i in range(94)) # 全角文字\n",
    "    HANKAKU = ''.join(chr(0x21 + i) for i in range(94)) # 半角文字\n",
    "    ZENTOHAN = str.maketrans(ZENKAKU, HANKAKU) # 全角→半角\n",
    "    HANTOZEN = str.maketrans(HANKAKU, ZENKAKU) # 半角→全角\n",
    "    ROUNDNUM = ''.join(chr(0x2460 + i) for i in range(9)) # 丸文字\n",
    "    NUM = ''.join(str(i) for i in range(1, 10)) # 数字\n",
    "    ROUNDTONUM = str.maketrans(ROUNDNUM, NUM) # 丸文字→数字\n",
    "    ADHOC_DICT = {'注': '※', '*': '※'} # 手動で整形する文字\n",
    "    ADHOC_TRANS = str.maketrans(ADHOC_DICT) # 変換テーブル\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        \n",
    "\n",
    "    def zentohan(self, zentohan=True):\n",
    "        if zentohan is True:\n",
    "            self.value = self.value.translate(self.ZENTOHAN)\n",
    "            return self\n",
    "        elif zentohan is False:\n",
    "            self.value = self.value.translate(self.HANTOZEN)\n",
    "            return self\n",
    "        else:\n",
    "            print('変換失敗')\n",
    "            return None\n",
    "\n",
    "\n",
    "    def roundtonum(self):\n",
    "        self.value = self.value.translate(self.ROUNDTONUM)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def adhoc_trans(self):\n",
    "        self.value = self.value.translate(self.ADHOC_TRANS)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_td(td):\n",
    "    td_text = td.text\n",
    "    fix = Fix(td_text)\n",
    "    td_fixed = fix.fix_value().value\n",
    "\n",
    "    return td_fixed\n",
    "\n",
    "\n",
    "def fix_tr(tr):\n",
    "    tds = tr.findAll('td')\n",
    "    tr_fixed = list(map(fix_td, tds))\n",
    "\n",
    "    return tr_fixed\n",
    "\n",
    "\n",
    "def div_to_df(div):\n",
    "    trs = div.findAll('tr')\n",
    "    trs_fixed = list(map(fix_tr, trs))\n",
    "    df = pd.DataFrame(trs_fixed)\n",
    "    df = df.replace('', np.nan)\n",
    "    df.dropna(axis=0, how='all', inplace=True)\n",
    "    df.set_axis(df.iloc[0, :], inplace=True, axis=1)\n",
    "    df = df.iloc[1:, :]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fix:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def fix_value(self):\n",
    "        value_split = self.value.splitlines()\n",
    "        value_join = ''.join(value_split)\n",
    "        translate = Translate(value_join)\n",
    "        value_fixed = translate.zentohan().roundtonum().adhoc_trans().value # Translateオブジェクトのvalue変数(str)を抽出\n",
    "        value_fixed = value_fixed.replace(' ', '').replace('　', '').replace('\\xa0', '').replace('\\u3000', '')\n",
    "        self.value = value_fixed\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def fix_annotation_pre(self):\n",
    "        pattern_pre = r'^[0-9]\\.|^[0-9]|^\\(※\\)|^※|^※[0-9]|^\\(※\\)[0-9]\\.|\\(※[0-9]\\)|^\\(|^\\)|、[0-9]'\n",
    "        repl = ''\n",
    "        annot_fixed = re.sub(pattern=pattern_pre, repl=repl, string=self.value, count=10)\n",
    "        self.value = annot_fixed\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def fix_annotation_suff(self):\n",
    "        pattern_suff = r'[0-9]$|[0-9]\\.$|、$|※[0-9]$'\n",
    "        repl = ''\n",
    "        annot_fixed = re.sub(pattern=pattern_suff, repl=repl, string=self.value, count=10)\n",
    "        self.value = annot_fixed\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def fix_annotation_adhoc(self, pattern, repl):\n",
    "        \"\"\"\n",
    "        指定した文字を指定した文字に置き換える関数\n",
    "        \"\"\"\n",
    "        annot_list = [re.sub(pattern=pattern, repl=repl, string=a) for a in self.value]\n",
    "        self.value = annot_list\n",
    "\n",
    "        return self        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation_original_to_df(html):\n",
    "    # 注釈文章をpタグで分割したままのデータフレームを出力\n",
    "    soup  = BeautifulSoup(html, 'html.parser')\n",
    "    div_last = soup.findAll('div')[-1]\n",
    "    annotation = div_last.findNextSiblings() #<p>タグ（等）が個数分だけ入る\n",
    "    annot_list = [re.sub(r'[0-9] ', r'[0-9]', Translate(a.text).roundtonum().value.rstrip(' ').rstrip('　').replace('\\u3000', '').replace('\\xa0', ''))\n",
    "        for a in annotation]\n",
    "    annot_list = [a for a in annot_list if a]\n",
    "    df = pd.DataFrame(annot_list).T\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = df_stk['edinet_code']\n",
    "names = df_stk['firm_name']\n",
    "ymds = df_stk['filling_ymd']\n",
    "htmls = df_stk['stk_html']\n",
    "annot_list = [[c, n, y, annotation_original_to_df(h)] for c, n, y, h in zip(codes, names, ymds, htmls)]\n",
    "\n",
    "annot_add_list = []\n",
    "for edinet_code, firm_name, filling_ymd, df in annot_list:\n",
    "    df.insert(loc=0, column='edinet_code', value=edinet_code)\n",
    "    df.insert(loc=1, column='firm_name', value=firm_name)\n",
    "    df.insert(loc=2, column='filling_ymd', value=filling_ymd)\n",
    "    annot_add_list.append(df)\n",
    "\n",
    "annot_merged = pd.concat(annot_add_list, axis=0)\n",
    "annot_cols = ['edinet_code', 'firm_name', 'filling_ymd'] + ['a_' + str(i) for i in range(19)]\n",
    "annot_merged.columns = annot_cols\n",
    "\n",
    "annot_merged.sort_values(['a_0'], inplace=True)\n",
    "path = '../../output/annotation_table.csv'\n",
    "annot_merged.to_csv(path, encoding='cp932', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../output/annotation_table_sukoshi_fixed.csv'\n",
    "annot_sukoshi = pd.read_csv(path, encoding='cp932', header=0)\n",
    "\n",
    "row_list = []\n",
    "for row in annot_sukoshi.itertuples():\n",
    "    col_list = []\n",
    "    for num in range(0, len(annot_sukoshi.columns) + 1):\n",
    "        if num >= 6:\n",
    "            value = row[num]\n",
    "            if not '[0-9]' in str(value):\n",
    "                new_values = str(value).split()\n",
    "            else:\n",
    "                new_values = str(value).split('[0-9]')\n",
    "            col_list.append(new_values)\n",
    "    code = row[1]\n",
    "    append_list = [code] + [c for cl in col_list for c in cl]\n",
    "    append_list_fixed = [Fix(a).fix_value().fix_annotation_pre().fix_annotation_pre().fix_annotation_pre().value for a in append_list]\n",
    "    append_list_fixed = [a for a in append_list_fixed if not a == '' and (not a == 'nan') and (a is not None) and (not '自己株式' in a) and (not '新株予約権' in a) and (not '総数' in a) and not ('潜在' in a) and (not 'す。' in a)]\n",
    "    row_list.append(append_list_fixed)\n",
    "\n",
    "annot_new_df = pd.DataFrame(row_list)\n",
    "annot_new_df.dropna(how='all', axis=1, inplace=True)\n",
    "path = '../../output/annot_new_fixed.csv'\n",
    "annot_new_df.to_csv(path, encoding='cp932', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手動整形後のcsv読込\n",
    "path = '../../output/annot_new_fixed_full.csv'\n",
    "annot_fixed_full = pd.read_csv(path, encoding='cp932', header=0, index_col='edinet_code')\n",
    "annot_fixed_full.dropna(how='all', inplace=True, axis=1)\n",
    "\n",
    "cols = annot_fixed_full.columns\n",
    "cols = [c for c in cols if c.startswith('a_')]\n",
    "for col in cols:\n",
    "    annot_fixed_full[col] = annot_fixed_full[col].apply(lambda x: Fix(str(x)).fix_annotation_pre().fix_annotation_pre().fix_annotation_suff().fix_annotation_suff().value)\n",
    "\n",
    "path = '../../output/annot_comp.csv'\n",
    "annot_fixed_full.to_csv(path, encoding='cp932', header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation_to_df(html):\n",
    "    # 注釈HTMLを抽出する\n",
    "    soup  = BeautifulSoup(html, 'html.parser')\n",
    "    div_last = soup.findAll('div')[-1]\n",
    "    annotation = div_last.findNextSiblings() #<p>タグ（等）が個数分だけ入る\n",
    "    print(annotation)\n",
    "\n",
    "    # '特別利害関係者'など、必要な項目のみをピックアップする\n",
    "    annot_list = []\n",
    "    for a in annotation:\n",
    "        a_text = a.get_text('@')\n",
    "        a_split = [a for al in a_text.split('\\n') for a in re.split(r'[@]', al)]\n",
    "        annot_list.append(a_split)\n",
    "\n",
    "    print(annot_list)\n",
    "    #annot_pickuped = list(filter(pickup_annot, annot_list))\n",
    "\n",
    "    # 各項目について文字列の整形を行う\n",
    "    annot_fixed_list = [a for al in annot_list for a in al] # 半角等様々な整形\n",
    "    annot_fixed_list = [a for a in annot_fixed_list if a]\n",
    "    # print(annot_to_df)\n",
    "    annot_to_df = annot_fixed_list\n",
    "    df = pd.DataFrame(annot_to_df).T\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    pattern_brackets = r'(\\(.+\\))|(:.+)'\n",
    "    brackets = [re.search(pattern=pattern_brackets, string=a) for a in annot_fixed_list]\n",
    "    brackets = [b.group() for b in brackets if b]\n",
    "    annot_remove_brackets = [re.sub(pattern_brackets, repl='', string=a) for a in annot_fixed_list if not a == '']\n",
    "    annot_remove_brackets = [a for a in annot_remove_brackets if not a == '']\n",
    "    print(brackets)\n",
    "    print(annot_remove_brackets)\n",
    "    annot_to_df = []\n",
    "    #for a, b in zip(annot_remove_brackets, brackets):\n",
    "    #    annot_to_df.append(a)\n",
    "    #    annot_to_df.append(b)\n",
    "    annot_to_df = [''.join([a, b]) for a, b in zip(annot_remove_brackets, brackets)]\n",
    "    # if annot_to_df == []:\n",
    "    #     annot_to_df = annot_fixed_list\n",
    "    # 最終リストを作成（アドホックに削除する）\n",
    "    ad_pattern_list = [\n",
    "        r'\\(※\\)', r'\\(※[0-9]\\)', r'(※)[0-9]\\.', r'^※', r'^※[0-9]', r'^※[0-9]', \n",
    "        r'[0-9]$|、', r'^[0-9]\\.', r'^[0-9]', r'^\\.', r'^:',\n",
    "        r'。なおRed\\(※1\\)', r'。なおRed', r''\n",
    "        ]\n",
    "    for ad_pattern in ad_pattern_list:\n",
    "        annot_to_df = Fix(annot_to_df).fix_annotation_adhoc(pattern=ad_pattern, repl='').value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_to_df(df_stk['stk_html'][84])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_list = [annotation_to_df(d) for d in df_stk['stk_html']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_merged = pd.concat(annot_list, axis=0)\n",
    "path = 'C:/Users/koeci/Google ドライブ/MBA/ワークショップ/intermediate/annotation_table.csv'\n",
    "annot_merged.to_csv(path, encoding='cp932', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = df_stk['stk_html'][952]\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "div = soup.findAll('div')[-1]\n",
    "annotation = div.findNextSiblings()\n",
    "annot_text = [t.text for t in annotation]\n",
    "print(annotation)\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_to_df(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    divs = soup.findAll('div')\n",
    "    df_list = list(map(div_to_df, divs))\n",
    "    # print(df_list)\n",
    "    try:\n",
    "        df = pd.concat(df_list, axis=0)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    except:\n",
    "        return pd.DataFrame() # 空のデータフレームを返す\n",
    "\n",
    "    if len(df.columns) == 4:\n",
    "        return df\n",
    "    else:\n",
    "        return pd.DataFrame() # 空のデータフレームを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_to_df_2(html, num):\n",
    "    df_html_list = pd.read_html(html)\n",
    "    df_list = []\n",
    "    for df in df_html_list:\n",
    "        df.dropna(how='all', inplace=True)\n",
    "        col_len = len(df.columns)\n",
    "        df.set_axis(labels=list(range(col_len)), axis=1, inplace=True)\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        df_list.append(df)\n",
    "\n",
    "    df_all = pd.concat(df_list, axis=0)\n",
    "    df_all['firm_num'] = num\n",
    "    \n",
    "    if len(df_all.columns) == 5:\n",
    "        return df_all\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_to_df_2\n",
    "htmls = df_stk['stk_html']\n",
    "df_htmls = [table_to_df_2(html, i) for i, html in enumerate(htmls)]\n",
    "df_htmls_merged = pd.concat(df_htmls, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_to_df\n",
    "codes = df_stk['edinet_code']\n",
    "names = df_stk['firm_name']\n",
    "ymds = df_stk['filling_ymd']\n",
    "htmls = df_stk['stk_html']\n",
    "firm_infos = [[code, name, ymd, table_to_df(html)] for code, name, ymd, html in zip(codes, names, ymds, htmls)]\n",
    "path = 'C:/Users/koeci/Google ドライブ/MBA/ワークショップ/intermediate/html_tables.pickle'\n",
    "with open(path, mode='wb') as file:\n",
    "    pickle.dump(obj=firm_infos, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firm_infos[450][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_metadata(num, edinet_code, firm_name, filling_ymd, df):\n",
    "    tb = df\n",
    "\n",
    "    # EDINETコード等をdfに追加する\n",
    "    try:\n",
    "        tb.insert(loc=0, column='edinet_code', value=edinet_code)\n",
    "        tb.insert(loc=1, column='firm_name', value=firm_name)\n",
    "        tb.insert(loc=2, column='filling_ymd', value=filling_ymd)\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 列数や企業番号を列に追加する\n",
    "    tb['row'] = tb.index\n",
    "    tb['num'] = num\n",
    "    new_colname = ['edinet_code', 'firm_name', 'filling_ymd', 'name', 'loc', 'stock', 'rates', 'row', 'num']\n",
    "    tb.set_axis(new_colname, inplace=True, axis=1)\n",
    "\n",
    "    return tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompose_df(df):\n",
    "    tb = df\n",
    "\n",
    "    def check_additional_info(value):\n",
    "        pattern = r'\\(([0-9]|.)+\\)'# (4.10)のような括弧の中に数字か'.'のみが入っているパターンは注釈なので位置行上にずらす\n",
    "        if pd.isna(value):\n",
    "            return 1\n",
    "        elif re.match(pattern=pattern, string=value):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # [rateがNAの行]=[名前の直後に（）のような株数の記載がある行]に目印を入れる\n",
    "    tb['r_na'] = tb['rates'].apply(check_additional_info) # rateがNAだったら１を入れる\n",
    "    tb_r_na = tb[tb['r_na'] == 1].copy() # rateがNAの行のみを抜き出す\n",
    "    tb_r_na['row'] = tb_r_na['row'].apply(lambda x: x - 1) # １行上=（）の氏名や住所が対応する行番号を取得する\n",
    "    r_na_colname = ['edinet_code', 'firm_name', 'filling_ymd', 'tmp1', 'tmp2', 'tmp3', 'tmp4', 'row', 'num', 'r_na']\n",
    "    tb_r_na.set_axis(r_na_colname, inplace=True, axis=1)\n",
    "\n",
    "    # （）内を該当行の直後の行から、横に持ってきて連結する\n",
    "    tb = tb[tb['r_na'] == 0] # （）が別行に入っていない行を抽出する\n",
    "    new_tb = pd.merge(left=tb, right=tb_r_na, on=['edinet_code', 'firm_name', 'filling_ymd', 'num', 'row'], how='left') \n",
    "\n",
    "    return new_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/koeci/Google ドライブ/MBA/ワークショップ/intermediate/html_tables.pickle'\n",
    "with open(path, mode='rb') as file:\n",
    "    firm_infos = pickle.load(file)\n",
    "tb_list = [add_metadata(num, fi[0], fi[1], fi[2], fi[3]) for num, fi in enumerate(firm_infos) if not len(fi[3]) == 0]\n",
    "tb_recomposed_list = [recompose_df(df) for df in tb_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat(tb_recomposed_list, axis=0)\n",
    "merged.sort_values(['num', 'row'], ascending=[True, True], inplace=True)\n",
    "\n",
    "merged_copy = merged.copy()\n",
    "tmp1 = merged_copy['tmp1']\n",
    "tmp2 = merged_copy['tmp2']\n",
    "tmp3 = merged_copy['tmp3']\n",
    "tmp4 = merged_copy['tmp4']\n",
    "new_tmp1 = []\n",
    "new_tmp2 = []\n",
    "new_tmp3 = []\n",
    "new_tmp4 = []\n",
    "\n",
    "regex = r'\\([0-9]|,+\\)|[0-9]|,+\\([0-9]|,+\\)'\n",
    "pattern = re.compile(regex)\n",
    "for t1, t2, t3, t4 in zip(tmp1, tmp2, tmp3, tmp4):\n",
    "    try:\n",
    "        matchObj = pattern.match(t1)\n",
    "        if not matchObj == None:\n",
    "            new_t3 = t2\n",
    "            new_t2 = t1\n",
    "            new_t1 = np.nan\n",
    "        else:\n",
    "            new_t3 = t3\n",
    "            new_t2 = t2\n",
    "            new_t1 = t1\n",
    "    except:\n",
    "        new_t3 = t3\n",
    "        new_t2 = t2\n",
    "        new_t1 = t1\n",
    "    \n",
    "    try:\n",
    "        matchObj = pattern.match(t2)\n",
    "        if not matchObj == None:\n",
    "            new_t4 = new_t3\n",
    "            new_t3 = new_t2\n",
    "            new_t2 = np.nan\n",
    "        else:\n",
    "            new_t4 = t4\n",
    "            new_t3 = t3\n",
    "            new_t2 = t2\n",
    "    except:\n",
    "        new_t4 = t4\n",
    "        new_t3 = t3\n",
    "        new_t2 = t2\n",
    "\n",
    "    new_tmp1.append(new_t1)\n",
    "    new_tmp2.append(new_t2)\n",
    "    new_tmp3.append(new_t3)\n",
    "    new_tmp4.append(new_t4)\n",
    "\n",
    "\n",
    "new_merged = merged.copy()\n",
    "new_merged['tmp1'] = new_tmp1\n",
    "new_merged['tmp2'] = new_tmp2\n",
    "new_merged['tmp3'] = new_tmp3\n",
    "new_merged['tmp4'] = new_tmp4\n",
    "#546, 16\n",
    "# 4000<500>を4000(500)にする\n",
    "new_merged['stock'] = [nm.replace('<', '(').replace('>', ')') for nm in new_merged['stock']]\n",
    "new_merged['rates'] = [nm.replace('<', '(').replace('>', ')') for nm in new_merged['rates']]\n",
    "\n",
    "# 名前の分割\n",
    "name_split = new_merged['name'].copy().apply(lambda x: str(x).split('※', maxsplit=1)[0])\n",
    "annot = new_merged['name'].copy().apply(lambda x: x.split('※', maxsplit=1)[1] if len(str(x).split('※')) >= 2 else np.nan)\n",
    "new_merged['name'] = name_split\n",
    "new_merged['tmp1'] = annot\n",
    "\n",
    "# 株数の分割\n",
    "stock_split = new_merged['stock'].copy().apply(lambda x: str(x).split('(', maxsplit=1)[0])\n",
    "stock_brackets = new_merged['stock'].copy().apply(lambda x: x.split('(', maxsplit=1)[1] if len(str(x).split('(')) >= 2 else np.nan)\n",
    "new_merged['stock'] = stock_split\n",
    "new_merged['tmp3'] = stock_brackets\n",
    "\n",
    "# 比率の分割\n",
    "rates_split = new_merged['rates'].copy().apply(lambda x: str(x).split('(', maxsplit=1)[0])\n",
    "rates_brackets = new_merged['rates'].copy().apply(lambda x: x.split('(', maxsplit=1)[1] if len(str(x).split('(')) >= 2 else np.nan)\n",
    "new_merged['rates'] = rates_split\n",
    "new_merged['tmp4'] = rates_brackets\n",
    "\n",
    "# 両端の括弧等を取る\n",
    "for col in new_merged.columns:\n",
    "    new_merged[col] = new_merged[col].apply(lambda x: str(x).strip(r'\\(').strip(r'\\)').replace('※', '').replace('、', ','))\n",
    "\n",
    "# それぞれの列でおかしな所を修正する\n",
    "new_merged['stock'] = new_merged['stock'].apply(lambda x: x.replace(',', '').replace('普通株式', ''))\n",
    "new_merged['rates'] = new_merged['rates'].apply(lambda x: x.replace(',', ''))\n",
    "new_merged['tmp1'] = new_merged['tmp1'].apply(lambda x: x.replace('.', ',').replace('()', ',').replace(')(', ',').strip(','))\n",
    "\n",
    "# tmp1列には数字かカンマ以外残さない関数\n",
    "def degit_or_comma(value):\n",
    "    find = re.findall(r'\\d+', str(value))\n",
    "    result = ','.join(find)\n",
    "    if result == '':\n",
    "        result = np.nan\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# 2桁の注釈番号にカンマを付ける関数\n",
    "def comma(value):\n",
    "    value_splited = []\n",
    "    s = str(value)\n",
    "    if str.isnumeric(s) and len(s) >= 2 and not s == '10' and not s == '11' and not s == '12' and not s == '13' and not s == '14':\n",
    "        value_splited = [v for v in s]\n",
    "        result = ','.join(value_splited)\n",
    "    else:\n",
    "        result = value\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "new_merged['tmp1'] = new_merged['tmp1'].apply(comma)\n",
    "new_merged['tmp1'] = new_merged['tmp1'].apply(degit_or_comma)\n",
    "new_merged['tmp3'] = new_merged['tmp3'].apply(lambda x: x.replace(',', ''))\n",
    "new_merged['tmp4'] = new_merged['tmp4'].apply(lambda x: x.replace(',', ''))\n",
    "\n",
    "# 注釈番号を区切って横に並べたデータ\n",
    "annot_numbers = new_merged['tmp1']\n",
    "annot_numbers_splited = list(map(lambda x: str(x).split(','), annot_numbers))\n",
    "annot_numbers_splited = list(map(pd.unique, annot_numbers_splited))\n",
    "annot_numbers_df = pd.DataFrame(annot_numbers_splited)\n",
    "cols = ['a_num_' + str(i + 1) for i in range(5)]\n",
    "annot_numbers_df.set_axis(labels=cols, axis=1, inplace=True)\n",
    "\n",
    "new_merged = pd.merge(left=new_merged, right=annot_numbers_df, left_index=True, right_index=True, how='left')\n",
    "path = '../../intermediate/stockholders_table_merged_1016.csv'\n",
    "new_merged.to_csv(path, encoding='utf-8', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDINETコードと証券コードの対応表を読み込み\n",
    "path = '../../data/EDINET/code_list/EdinetcodeDlInfo.csv'\n",
    "syoken_df = pd.read_csv(path, encoding='cp932', skiprows=1, header=0)\n",
    "syoken_df = syoken_df[['ＥＤＩＮＥＴコード', '証券コード']]\n",
    "syoken_df.set_axis(labels=['edinet_code', 'security_code'], axis=1, inplace=True)\n",
    "syoken_df['security_code'] = syoken_df['security_code'].apply(lambda x: str(x)[0:4] if x else np.nan)\n",
    "syoken_df.sort_values('security_code', inplace=True)\n",
    "path = '../../data/EDINET/code_list/edinet_security_code.csv'\n",
    "syoken_df.to_csv(path, encoding='cp932', header=True, index=False)\n",
    "\n",
    "path = '../../data/EDINET/code_list/edinet_security_code.csv'\n",
    "df_security = pd.read_csv(path, encoding='cp932', header=0, index_col=0, dtype={'security_code': str})\n",
    "\n",
    "# 注釈テーブルの読込\n",
    "path = '../../output/annot_comp.csv'\n",
    "annot_fixed_full = pd.read_csv(path, encoding='cp932', header=0, index_col=0)\n",
    "\n",
    "# 株主テーブルの読込\n",
    "path = '../../intermediate/stockholders_table_merged_1016.csv'\n",
    "new_merged = pd.read_csv(path, encoding='utf-8', header=0, index_col=0)\n",
    "\n",
    "# すべてを１つのテーブルに収める\n",
    "df_comp = pd.merge(left=new_merged, right=df_security, left_index=True, right_index=True, how='left')\n",
    "df_comp = pd.merge(left=df_comp, right=annot_fixed_full, left_index=True, right_index=True, how='left')\n",
    "cols_comp = [\n",
    "    'firm_name', 'filling_ymd', 'stockholder_name', 'loc', 'stock', 'stock_rates', 'table_row', 'table_num', 'r_na_x', 'name_infos', 'loc_infos',\n",
    "    'stock_infos', 'stock_rates_infos', 'r_na_y' \n",
    "]\n",
    "a_num_cols = annot_numbers_df.columns\n",
    "a_text_cols = [c for c in annot_fixed_full.columns if c.startswith('a_')]\n",
    "cols_comp = [cols_comp] + [a_num_cols] + [['security_code']] + [a_text_cols]\n",
    "cols_comp = [c for cl in cols_comp for c in cl]\n",
    "df_comp.set_axis(labels=cols_comp, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "sort_cols = [\n",
    "    'security_code', 'firm_name', 'filling_ymd', 'stockholder_name', 'loc', 'stock',\n",
    "    'stock_rates', 'stock_infos', 'stock_rates_infos', 'a_num_1',\n",
    "    'a_num_2', 'a_num_3', 'a_num_4', 'a_num_5', 'a_1',\n",
    "    'a_2', 'a_3', 'a_4', 'a_5', 'a_6', 'a_7', 'a_8', 'a_9', 'a_10', 'a_11',\n",
    "    'a_12', 'a_13', 'a_14', 'a_15', 'a_16', 'a_17'\n",
    "    ]\n",
    "\n",
    "df_comp = df_comp[sort_cols]\n",
    "\n",
    "# 注釈番号と注釈文章を対応させる\n",
    "row_list = []\n",
    "for row in df_comp.itertuples():\n",
    "    col_list = []\n",
    "    for j in range(1, 5):\n",
    "        annot_num = row[j + 9]\n",
    "        if type(annot_num) == float and annot_num == annot_num and annot_num is not None:\n",
    "            annot_text = row[int(annot_num) + 14]\n",
    "        else:\n",
    "            annot_text = None\n",
    "\n",
    "        col_list.append(annot_text)\n",
    "\n",
    "    row_list.append(col_list)\n",
    "\n",
    "annot_num_text_df = pd.DataFrame(row_list, index=df_comp.index)\n",
    "cols = ['a_text_' + str(i + 1) for i in range(len(annot_num_text_df.columns))]\n",
    "annot_num_text_df.columns = cols\n",
    "df_comp = pd.concat([df_comp, annot_num_text_df], axis=1)\n",
    "\n",
    "sort_cols = [\n",
    "    'security_code', 'firm_name', 'filling_ymd', 'stockholder_name', 'loc', 'stock',\n",
    "    'stock_rates', 'stock_infos', 'stock_rates_infos', 'a_text_1',\n",
    "    'a_text_2', 'a_text_3', 'a_text_4', 'a_num_1',\n",
    "    'a_num_2', 'a_num_3', 'a_num_4', 'a_1',\n",
    "    'a_2', 'a_3', 'a_4', 'a_5', 'a_6', 'a_7', 'a_8', 'a_9', 'a_10', 'a_11',\n",
    "    'a_12', 'a_13', 'a_14', 'a_15', 'a_16', 'a_17'\n",
    "    ]\n",
    "\n",
    "df_comp = df_comp[sort_cols]\n",
    "\n",
    "# 証券コードの補完\n",
    "path = '../../intermediate/securities_na.csv'\n",
    "edinet_security_table = pd.read_csv(path, encoding='cp932', header=0)\n",
    "table_dict = {e: s for e, s in zip(edinet_security_table['edinet_code'], edinet_security_table['security_code'])}\n",
    "\n",
    "# 証券コードNAとそれ以外で分割→補完→マージ\n",
    "df_comp_nona = df_comp.copy().query('security_code == security_code')\n",
    "df_comp_na = df_comp.copy().query('not security_code == security_code')\n",
    "fixed_security_code = list(map(lambda x: table_dict[x], df_comp_na.index))\n",
    "df_comp_na['security_code'] = fixed_security_code\n",
    "df_comp = pd.concat([df_comp_nona, df_comp_na], axis=0)\n",
    "\n",
    "path = '../../output/stockholders_1016.csv'\n",
    "df_comp.to_csv(path, encoding='utf-8', header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../output/stockholders_1016.csv'\n",
    "df_comp = pd.read_csv(path, encoding='utf-8', header=0, index_col=0)\n",
    "\n",
    "# 証券コードでソート\n",
    "df_comp.sort_values('security_code', inplace=True)\n",
    "\n",
    "# 全体テーブルの出力\n",
    "path = '../../output/full_data_20211016.csv'\n",
    "df_comp.to_csv(path, encoding='utf-8', header=True, index=True)\n",
    "\n",
    "# 株主テーブルの出力\n",
    "path = '../../output/stockholders_20211016.csv'\n",
    "df_stockholders = df_comp[[\n",
    "    'security_code', 'firm_name', 'filling_ymd', 'stockholder_name', 'loc', 'stock',\n",
    "    'stock_rates', 'stock_infos', 'stock_rates_infos',\n",
    "    'a_num_1', 'a_num_2', 'a_num_3', 'a_num_4'\n",
    "]]\n",
    "df_stockholders.to_csv(path, encoding='utf-8', header=True, index=True)\n",
    "\n",
    "# 注釈テーブルの出力\n",
    "path = '../../output/annotation_20211016.csv'\n",
    "df_annotation = df_comp[[\n",
    "    'security_code', 'firm_name', 'filling_ymd', \n",
    "    'a_1', 'a_2', 'a_3', 'a_4', 'a_5', 'a_6', 'a_7',\n",
    "    'a_8', 'a_9', 'a_10', 'a_11', 'a_12', 'a_13', \n",
    "    'a_14', 'a_15', 'a_16', 'a_17'\n",
    "]]\n",
    "df_annotation.to_csv(path, encoding='cp932', header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 証券コードがNAの企業\n",
    "securities_na = df_comp.query('not security_code == security_code')\n",
    "securities_na = securities_na.drop_duplicates(subset='firm_name')\n",
    "securities_na.to_csv('../../intermediate/証券コードNA.csv', encoding='cp932', header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 企業数の確認\n",
    "path = '../../output/samples_20211016.csv'\n",
    "samples = df_comp.drop_duplicates(subset='firm_name').count()\n",
    "samples.to_csv(path, header=False, index=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "530756d384d84b166bf1cc6dadec8b1c5ff774bb470199997245af3e35454b18"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
