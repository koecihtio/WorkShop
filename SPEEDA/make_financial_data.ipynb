{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 財務データの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) 企業データを年で縦に結合する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 各年を接合する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各年の結合\n",
    "common = '../../data/SPEEDA/CompanyList_1018/CompanyList_'\n",
    "cols = [\n",
    "    'stock_code', 'corporate_number', 'firm_name', 'fiscal_year', 'status', 'establishment', 'prefecture',\n",
    "    'ceo', 'accounting', 'sales', 'operating_profit', 'net_profit', 'total_assets', 'cash',\n",
    "    'fixed_assets', 'total_liabilities', 'current_liabilities', 'stockholders_equity', 'other_gains',\n",
    "    'operating_cash_flow', 'stock_price', 'extraordinary_loss', 'foreign_stock_ratio'\n",
    "    ]\n",
    "df_list = []\n",
    "for year in range(1998, 2021):\n",
    "    path = common + str(year) + '.xlsx'\n",
    "    df = pd.read_excel(path, sheet_name='企業リスト', skiprows=7, header=0)\n",
    "    df.dropna(subset=['企業名称'], how='any', inplace=True) # 空行等を削除する\n",
    "    df.set_axis(labels=cols, axis=1, inplace=True)\n",
    "    df_list.append(df)\n",
    "    # print(f'{year} was successfully closed')\n",
    "\n",
    "df = pd.concat(df_list, axis=0) # 縦に結合\n",
    "df.dropna(subset=['fiscal_year'], inplace=True) # 不要行の削除\n",
    "df = df.assign( # 列の更新・修正（新規追加含む）\n",
    "    year = df['fiscal_year'].apply(lambda x: str(x)[0:4]),\n",
    "    month = df['fiscal_year'].apply(lambda x: str(x)[5:7]),\n",
    "    establishment = df['establishment'].apply(lambda x: str(x)[0:4] if not x == '-' else np.nan),\n",
    "    ceo = df['ceo'].apply(lambda x: x if not x == '-' else np.nan),\n",
    "    prefecture = df['prefecture'].apply(lambda x: x if not x == '-' else np.nan),\n",
    "    accounting_code = df['accounting'].astype('category').cat.codes.replace(-1, np.nan), # 会計基準コードの付与\n",
    "    fixed_assets = df['fixed_assets'].fillna(0),\n",
    "    stockholders_equity = df['stockholders_equity'].fillna(0),\n",
    "    other_gains = df['other_gains'].fillna(0),\n",
    "    operating_cash_flow = df['operating_cash_flow'].fillna(0),\n",
    "    extraordinary_loss = df['extraordinary_loss'].fillna(0),\n",
    ")\n",
    "\n",
    "# 経営者データの更新\n",
    "path = '../../data/eol/ceo_data.csv'\n",
    "ceo_data = pd.read_csv(path, encoding='cp932', header=0)\n",
    "ceo_data = ceo_data[['証券コード', 'データ年', '代表者名']]\n",
    "ceo_data.set_axis(['stock_code', 'year', 'ceo_new'], inplace=True, axis=1)\n",
    "ceo_data = ceo_data.astype({\n",
    "    'stock_code': 'str',\n",
    "    'year': 'str',\n",
    "    'ceo_new': 'str'\n",
    "})\n",
    "df = pd.merge(left=df, right=ceo_data, on=['stock_code', 'year'], how='left')\n",
    "df['ceo'] = df['ceo_new']\n",
    "\n",
    "# 業種の付与\n",
    "path = '../../data/TDnet/証券コードリスト/syoken_code.xls'\n",
    "ind_data = pd.read_excel(path, sheet_name='Sheet1', header=0)\n",
    "ind_data.columns = [\n",
    "    'ymd', 'stock_code', 'firm_name', 'market', 'indcode_small', 'indname_small', 'indcode_large',\n",
    "    'indname_large', 'scale_code', 'scale_category'\n",
    "    ]\n",
    "ind_data = ind_data[['stock_code', 'indcode_small', 'indname_small', 'indcode_large', 'indname_large']]\n",
    "ind_data['stock_code'] = ind_data['stock_code'].apply(str)\n",
    "for col in ['indcode_small', 'indname_small', 'indcode_large', 'indname_large']:\n",
    "    ind_data[col] = ind_data[col].apply(lambda x: x if not x == '-' else np.nan)\n",
    "df = pd.merge(left=df, right=ind_data, on='stock_code', how='left')\n",
    "\n",
    "# 都道府県コードの付与\n",
    "path = '../../data/Prefecture/prefecture_code.csv'\n",
    "prefecture_code = pd.read_csv(path, encoding='cp932', header=0)\n",
    "df = pd.merge(left=df, right=prefecture_code, on='prefecture', how='left')\n",
    "\n",
    "# 感応度分析有無の付与\n",
    "path = '../../data/eol/sensitivity/sensitivity_all.csv'\n",
    "sensitivity = pd.read_csv(path, encoding='utf_8_sig', header=0)\n",
    "sensitivity = sensitivity[['stock_code', 'year', 'sensitivity']]\n",
    "sensitivity['stock_code'] = sensitivity['stock_code'].apply(str)\n",
    "df = pd.merge(left=df, right=sensitivity, on=['stock_code', 'year'], how='left')\n",
    "df['sensitivity'] = df['sensitivity'].fillna(0)\n",
    "\n",
    "# BCPの付与\n",
    "path = 'C:/Users/koeci/Google ドライブ/MBA/ワークショップ/data/eol/BCP/securities/BCP_data.csv'\n",
    "bcp = pd.read_csv(path, encoding='cp932', header=0)\n",
    "bcp = bcp[['stock_code', 'year', 'BCP', 'BCP_first_year']] # 列を選択\n",
    "bcp['stock_code'] = bcp['stock_code'].apply(str) # 型変換\n",
    "bcp['year'] = bcp['year'].apply(str) # 型変換\n",
    "df = pd.merge(left=df, right=bcp, on=['stock_code', 'year'], how='left')\n",
    "\n",
    "## 初めてBCPを策定した年以降にBCPを策定していることを表す１を付与する\n",
    "df_grouped = df.groupby('stock_code')['BCP_first_year'].min()\n",
    "df_grouped.name = 'fy'\n",
    "df = pd.merge(left=df, right=df_grouped, on='stock_code', how='left')\n",
    "df['BCP_first'] = [1 if int(y) == int(y_bcp) else 0 for y, y_bcp in zip(df['year'], df['fy'].fillna(0))]\n",
    "df['BCP_dev'] = [1 if int(y) >= int(y_bcp) else 0 for y, y_bcp in zip(df['year'], df['fy'].fillna(10000))] # nanを一時的に補完（大きい数なら何でも良い）\n",
    "df.drop(columns=['BCP_first_year', 'fy'], inplace=True)\n",
    "\n",
    "## nanを0で埋める\n",
    "for col in ['BCP', 'BCP_dev']:\n",
    "    df[col] = df[col].fillna(0)\n",
    "\n",
    "# 列名でソート\n",
    "cols_sort = [\n",
    "    'stock_code', 'firm_name', 'year', 'month', 'establishment', 'ceo', 'accounting', 'accounting_code', 'prefecture', 'prefecture_code',\n",
    "    'indcode_small', 'indname_small', 'indcode_large', 'indname_large', 'sales', 'operating_profit',\n",
    "    'net_profit', 'total_assets', 'cash', 'fixed_assets', 'total_liabilities', 'current_liabilities',\n",
    "    'stockholders_equity', 'other_gains', 'operating_cash_flow', 'stock_price', 'extraordinary_loss', 'foreign_stock_ratio', 'sensitivity',\n",
    "    'BCP', 'BCP_first', 'BCP_dev'\n",
    "]\n",
    "df = df.loc[:, cols_sort]\n",
    "df.sort_values(['stock_code', 'year'], inplace=True)\n",
    "\n",
    "# 保存(unicode error回避のためignoreを設定する)\n",
    "path = '../../data/SPEEDA/CompanyList_1018/CompanyListBinded.csv'\n",
    "with open(path, mode='w', encoding='cp932', errors='ignore') as file:\n",
    "    df.to_csv(file, header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1期前のデータを横にくっつける"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/SPEEDA/CompanyList_1018/CompanyListBinded.csv'\n",
    "df_pre = pd.read_csv(path, encoding='cp932', header=0)\n",
    "df_grouped = df_pre.groupby('stock_code')\n",
    "cols_pre = [\n",
    "    'year', 'month', 'establishment', 'ceo', 'accounting', 'accounting_code', 'prefecture', 'prefecture_code',\n",
    "    'sales', 'operating_profit', 'net_profit', 'total_assets', 'cash', 'fixed_assets', 'total_liabilities',\n",
    "    'current_liabilities', 'stockholders_equity', 'other_gains', 'operating_cash_flow', 'stock_price',\n",
    "    'extraordinary_loss', 'foreign_stock_ratio', 'sensitivity', 'BCP', 'BCP_first', 'BCP_dev'\n",
    "]\n",
    "for col in cols_pre:\n",
    "    new_col = 'lag_' + col\n",
    "    df_pre[new_col] = df_grouped[col].shift(1)\n",
    "\n",
    "path = '../../data/SPEEDA/CompanyList_1018/CompanyListBinded_withPre.csv'\n",
    "df_pre.to_csv(path, encoding='cp932', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) 経営指標を作成する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 指標の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "path = '../../data/SPEEDA/CompanyList_1018/CompanyListBinded_withPre.csv'\n",
    "df = pd.read_csv(path, header=0, encoding='cp932')\n",
    "\n",
    "# 指標作成に当たり、割り算の分母が０になるのを未然に防ぐ\n",
    "cols = [\n",
    "    'sales', 'operating_profit', 'operating_cash_flow', 'total_assets', 'stockholders_equity',\n",
    "    'other_gains', 'lag_stock_price', 'lag_net_profit'\n",
    "    ]\n",
    "for col in cols:\n",
    "    df[col] = df[col].apply(lambda x: x if not x == 0 else np.nan)\n",
    "\n",
    "# 指標の作成\n",
    "df_b = df.assign(\n",
    "    b_sales_growth = (df['sales'] - df['lag_sales']) / df['lag_sales'],\n",
    "    b_total_assets = df['total_assets'],\n",
    "    b_operating_cash_flow = df['operating_cash_flow'],\n",
    "    b_ros = df['operating_profit'] / df['sales'],\n",
    "    b_cash_deposit_ratio = df['cash'] / (df['stockholders_equity'] + df['other_gains']),\n",
    "    b_leverage = df['total_assets'] / (df['stockholders_equity'] + df['other_gains']),\n",
    "    b_stock_price_growth = (df['stock_price'] - df['lag_stock_price']) / df['lag_stock_price'],\n",
    "    b_net_profit_growth = (df['net_profit'] - df['lag_net_profit']) / df['lag_net_profit'],\n",
    "    b_firm_age = df['year'] - df['establishment'],\n",
    "    b_fixed_assets_ratio = df['fixed_assets'] / (df['stockholders_equity'] + df['other_gains']),\n",
    "    b_extraordinary_loss = df['extraordinary_loss'],\n",
    "    b_foreign_stock_ratio = df['foreign_stock_ratio'],\n",
    "    b_earthquake = [1 if (y >= 2012) | ((y == 2011) & (m >= 4)) else 0 for y, m in zip(df['year'], df['month'])],\n",
    "    b_covid_19 = [1 if ((y == 2020) & (m >= 2)) | (y >= 2021) else 0 for y, m in zip(df['year'], df['month'])],\n",
    "    b_turnover = [1 if not ceo == lag_ceo else 0 for ceo, lag_ceo in zip(df['ceo'], df['lag_ceo'])],\n",
    "    b_sensitivity_analysis = df['sensitivity']\n",
    ")\n",
    "\n",
    "# 経営者交代の計測に前期分を使うので、前期がない行を落とす\n",
    "df_b.dropna(subset=['lag_ceo'], inplace=True)\n",
    "\n",
    "# 計算できなかった指標をNAにする\n",
    "df_b['b_firm_age'] = df_b['b_firm_age'].apply(lambda x: x if x >= 0 else np.nan)\n",
    "# df_b['b_net_profit_ratio'] = df_b['b_net_profit_ratio'].apply(lambda x: x if not np.isinf(x) else np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAなどの処理&グループ化計算が必要な指標の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3124\n"
     ]
    }
   ],
   "source": [
    "# NaNは全て落とす\n",
    "df_b.dropna(how='any', inplace=True)\n",
    "\n",
    "# 異常値処理\n",
    "def remove_outliers(df, col, q_min=0, q_max=1):\n",
    "    lower = df[col].quantile(q_min)\n",
    "    upper = df[col].quantile(q_max)\n",
    "    df = df.query(f'{lower} <= {col} & {col} <= {upper}')\n",
    "\n",
    "    return df\n",
    "\n",
    "cols = ['b_operating_cash_flow', 'b_fixed_assets_ratio']\n",
    "for col in cols:\n",
    "    df_b = remove_outliers(df_b, col, q_min=0.005, q_max=0.995)\n",
    "    \n",
    "# 経営者累積交代回数の計測\n",
    "df_b['b_turnover_cumsum'] = df_b.groupby('stock_code').cumsum()['b_turnover']\n",
    "\n",
    "# 感応度分析累積実施回数の計測\n",
    "df_b['b_sensitivity_analysis_cumsum'] = df_b.groupby('stock_code').cumsum()['b_sensitivity_analysis']\n",
    "\n",
    "# BCP累計開示回数の計測\n",
    "df_b['b_BCP_cumsum'] = df_b.groupby('stock_code').cumsum()['BCP']\n",
    "\n",
    "# 1期前の指標を横にくっつける\n",
    "cols = [c for c in df_b.columns if c.startswith('b_')]\n",
    "df_grouped = df_b.groupby('stock_code')\n",
    "for col in cols:\n",
    "    new_col = 'lag_' + col\n",
    "    df_b[new_col] = df_grouped[col].shift(1)\n",
    "\n",
    "df_b.dropna(how='any', inplace=True)\n",
    "\n",
    "# 年数が足りない企業＆年が飛んでいる企業を落とす\n",
    "df_grouped = df_b.groupby('stock_code').agg(['count', 'min']) # 企業ごとのレコード数をカウント\n",
    "df_grouped = df_grouped['year'] # year列を抽出（これをしないとエラー）\n",
    "df_b = pd.merge(left=df_b, right=df_grouped, how='left', on='stock_code') # 元データにカウント数をマージする\n",
    "df_b['row'] = df_b.groupby('stock_code').cumcount() # グループごとに連番を振る\n",
    "df_b['correct_year'] = df_b['min'] + df_b['row'] # 本来あるべき年\n",
    "df_b = df_b.query('year == correct_year') # 年が飛んでいる企業を落とす\n",
    "\n",
    "print(df_b['stock_code'].drop_duplicates().count()) # 企業数（ユニーク）の表\n",
    "\n",
    "# 保存\n",
    "path = '../../data/SPEEDA/CompanyList_1018/CompanyListProcessed.csv'\n",
    "df_b.to_csv(path, header=True, index=False, encoding='cp932')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yeo-Johnson変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['stock_code', 'firm_name', 'year', 'month', 'establishment',\n",
      "       'prefecture', 'prefecture_code', 'ceo', 'accounting', 'accounting_code',\n",
      "       'indcode_small', 'indname_small', 'indcode_large', 'indname_large',\n",
      "       'yj_sales_growth', 'yj_total_assets', 'yj_operating_cash_flow',\n",
      "       'yj_ros', 'yj_cash_deposit_ratio', 'yj_leverage',\n",
      "       'yj_stock_price_growth', 'yj_net_profit_growth', 'yj_firm_age',\n",
      "       'yj_fixed_assets_ratio', 'yj_extraordinary_loss',\n",
      "       'yj_foreign_stock_ratio', 'lag_yj_sales_growth', 'lag_yj_total_assets',\n",
      "       'lag_yj_operating_cash_flow', 'lag_yj_ros', 'lag_yj_cash_deposit_ratio',\n",
      "       'lag_yj_leverage', 'lag_yj_stock_price_growth',\n",
      "       'lag_yj_net_profit_growth', 'lag_yj_firm_age',\n",
      "       'lag_yj_fixed_assets_ratio', 'lag_yj_extraordinary_loss',\n",
      "       'lag_yj_foreign_stock_ratio', 'earthquake', 'lag_earthquake',\n",
      "       'turnover', 'lag_turnover', 'turnover_cumsum', 'lag_turnover_cumsum',\n",
      "       'sensitivity_analysis', 'lag_sensitivity_analysis',\n",
      "       'sensitivity_analysis_cumsum', 'lag_sensitivity_analysis_cumsum',\n",
      "       'covid_19', 'lag_covid_19', 'BCP', 'lag_BCP', 'BCP_cumsum',\n",
      "       'lag_BCP_cumsum', 'BCP_dev', 'lag_BCP_dev', 'b_sales_growth'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def trans_yeo_johnson(series):\n",
    "    mm = MinMaxScaler()\n",
    "    pt = PowerTransformer(standardize=True)\n",
    "    data = series.values.reshape(-1, 1)\n",
    "    mm.fit(data)\n",
    "    pt.fit(data)\n",
    "    result = pt.transform(data)\n",
    "\n",
    "    return result\n",
    "\n",
    "cols = [c for c in df_b.columns if ('b_' in c)]\n",
    "for col in cols:\n",
    "    new_col = col.replace('b_', 'yj_')\n",
    "    df_b[new_col] = trans_yeo_johnson(df_b[col])\n",
    "    \n",
    "df_b.rename(columns={\n",
    "    'b_earthquake': 'earthquake', 'lag_b_earthquake': 'lag_earthquake',\n",
    "    'b_turnover': 'turnover', 'lag_b_turnover': 'lag_turnover',\n",
    "    'b_turnover_cumsum': 'turnover_cumsum', 'lag_b_turnover_cumsum': 'lag_turnover_cumsum',\n",
    "    'b_sensitivity_analysis': 'sensitivity_analysis', 'lag_b_sensitivity_analysis': 'lag_sensitivity_analysis',\n",
    "    'b_sensitivity_analysis_cumsum': 'sensitivity_analysis_cumsum', 'lag_b_sensitivity_analysis_cumsum': 'lag_sensitivity_analysis_cumsum',\n",
    "    'b_covid_19': 'covid_19', 'lag_b_covid_19': 'lag_covid_19',\n",
    "    'b_BCP_cumsum': 'BCP_cumsum', 'lag_b_BCP_cumsum': 'lag_BCP_cumsum'\n",
    "    }, inplace=True)\n",
    "cols = [\n",
    "    'stock_code', 'firm_name', 'year', 'month', 'establishment', 'prefecture', 'prefecture_code',\n",
    "    'ceo', 'accounting', 'accounting_code',\n",
    "    'indcode_small', 'indname_small', 'indcode_large', 'indname_large'\n",
    "    ]\n",
    "cols = cols \\\n",
    "    + [c for c in df_b.columns if ('yj_' in c) & ~('earthquake' in c) & ~('turnover' in c) & ~('sensitivity' in c) & ~('covid_19' in c) & ~('BCP' in c)] \\\n",
    "    + ['earthquake', 'lag_earthquake', 'turnover', 'lag_turnover', 'turnover_cumsum', 'lag_turnover_cumsum', 'sensitivity_analysis', 'lag_sensitivity_analysis', \\\n",
    "    'sensitivity_analysis_cumsum', 'lag_sensitivity_analysis_cumsum', 'covid_19', 'lag_covid_19', \n",
    "    'BCP', 'lag_BCP', 'BCP_cumsum', 'lag_BCP_cumsum', 'BCP_dev', 'lag_BCP_dev', 'b_sales_growth']\n",
    "df_yj = df_b[cols]\n",
    "print(df_yj.columns)\n",
    "path = '../../data/Analysis/analysis_data.csv'\n",
    "df_yj.to_csv(path, header=True, index=False, encoding='cp932')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 決算期を３月に限定したver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/Analysis/analysis_data.csv'\n",
    "df = pd.read_csv(path, encoding='cp932', header=0)\n",
    "df = df.query('month == 3')\n",
    "df.dropna(how='any', inplace=True) # 影響なし\n",
    "path = '../../data/Analysis/analysis_data_three.csv'\n",
    "df.to_csv(path, encoding='cp932', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## サンプル数チェック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### 全期間 #####\n",
      "企業数：3124\n",
      "BCP策定企業数：1174\n",
      "感応度分析実施企業数：360\n",
      "経営者交代企業数：1985\n",
      "##### 3月 #####\n",
      "企業数：2119\n",
      "BCP策定企業数：926\n",
      "感応度分析実施企業数：308\n",
      "経営者交代企業数：1507\n"
     ]
    }
   ],
   "source": [
    "path = '../../data/Analysis/analysis_data.csv'\n",
    "df = pd.read_csv(path, encoding='cp932', header=0)\n",
    "path = '../../data/Analysis/analysis_data_three.csv'\n",
    "df_three = pd.read_csv(path, encoding='cp932', header=0)\n",
    "\n",
    "def sample_check(df):\n",
    "    firmnum = df.drop_duplicates(subset=['stock_code'])['stock_code'].count()\n",
    "    bcp_firmnum = df.query('BCP == 1').drop_duplicates(subset=['stock_code'])['stock_code'].count()\n",
    "    sensitivity_firmnum = df.query('sensitivity_analysis == 1').drop_duplicates(subset=['stock_code'])['stock_code'].count()\n",
    "    turnover_firmnum = df.query('turnover == 1').drop_duplicates(subset=['stock_code'])['stock_code'].count()\n",
    "    print(f'企業数：{firmnum}')\n",
    "    print(f'BCP策定企業数：{bcp_firmnum}')\n",
    "    print(f'感応度分析実施企業数：{sensitivity_firmnum}')\n",
    "    print(f'経営者交代企業数：{turnover_firmnum}')\n",
    "\n",
    "df_list = [df, df_three]\n",
    "annot = ['全期間', '3月']\n",
    "for d, a in zip(df_list, annot):\n",
    "    print(f'##### {a} #####')\n",
    "    sample_check(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの作成メモ\n",
    "- outlier: 分布が極端に偏る変数について、上下0.5%ずつの異常値を除去\n",
    "- year: 年数が飛んでいたり、５年以上の年数が確保できない企業を除去\n",
    "- standarlized：多くの変数で分布が極端に偏っているため、Yeo-Johnson変換を実施。Box-Cox変換と比べて、負の値を処理できる点から採用した。"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "530756d384d84b166bf1cc6dadec8b1c5ff774bb470199997245af3e35454b18"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
